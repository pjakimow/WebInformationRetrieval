{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with a document corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "document0 = 'preliminary finding in cancer research'\n",
    "document1 = 'novel cancer research findings'\n",
    "document2 = 'new research to heal cancer'\n",
    "document3 = 'healing novel cancer research'\n",
    "corpus = [document0, document1, document2, document3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Jaccard score for each of the pair of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccord_score(doc1, doc2): \n",
    "    words_1 = doc1.split()\n",
    "    words_2 = doc2.split()\n",
    "    unique_words_1 = list(set(words_1))\n",
    "    unique_words_2 = list(set(words_2))\n",
    "    \n",
    "    intersection = [value for value in unique_words_1 if value in unique_words_2]\n",
    "    union = list(set(unique_words_1 + unique_words_2))\n",
    "    return round(len(intersection) / len(union),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard score for document 0  and document 0  is:  1.0\n",
      "Jaccard score for document 0  and document 1  is:  0.286\n",
      "Jaccard score for document 0  and document 2  is:  0.25\n",
      "Jaccard score for document 0  and document 3  is:  0.286\n",
      "Jaccard score for document 1  and document 1  is:  1.0\n",
      "Jaccard score for document 1  and document 2  is:  0.286\n",
      "Jaccard score for document 1  and document 3  is:  0.6\n",
      "Jaccard score for document 2  and document 2  is:  1.0\n",
      "Jaccard score for document 2  and document 3  is:  0.286\n",
      "Jaccard score for document 3  and document 3  is:  1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    for j in range(len(corpus)):\n",
    "        if i<=j:\n",
    "            print('Jaccard score for document',i, ' and document',j,' is: ', jaccord_score(corpus[i],corpus[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(corpus):\n",
    "    words = []\n",
    "    for doc in corpus:\n",
    "        words += doc.split()\n",
    "        \n",
    "    return list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finding',\n",
       " 'cancer',\n",
       " 'findings',\n",
       " 'healing',\n",
       " 'new',\n",
       " 'to',\n",
       " 'novel',\n",
       " 'preliminary',\n",
       " 'in',\n",
       " 'heal',\n",
       " 'research']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = get_unique_words(corpus)\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(term, document):\n",
    "    \"\"\" the number of occurences of given term in given document\"\"\"\n",
    "    count = 0\n",
    "    for word in document.split():\n",
    "        if word == term:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf( finding , doc0):  1\n",
      "tf( cancer , doc0):  1\n",
      "tf( findings , doc0):  0\n",
      "tf( healing , doc0):  0\n",
      "tf( new , doc0):  0\n",
      "tf( to , doc0):  0\n",
      "tf( novel , doc0):  0\n",
      "tf( preliminary , doc0):  1\n",
      "tf( in , doc0):  1\n",
      "tf( heal , doc0):  0\n",
      "tf( research , doc0):  1\n"
     ]
    }
   ],
   "source": [
    "for word in unique_words:\n",
    "    print('tf(',word,', doc0): ', term_frequency(word, corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_vector(document, corpus):\n",
    "    \"\"\" Calculates the term frequency vector. \"\"\"   \n",
    "    unique_terms = get_unique_words(corpus)\n",
    "    \n",
    "    result = []\n",
    "    for term in unique_terms:\n",
    "        result += [term_frequency(term, document)]\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words are: \n",
      " ['finding', 'cancer', 'findings', 'healing', 'new', 'to', 'novel', 'preliminary', 'in', 'heal', 'research'] \n",
      "\n",
      "tf_vector(doc 0 ):  [1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
      "tf_vector(doc 1 ):  [0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]\n",
      "tf_vector(doc 2 ):  [0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1]\n",
      "tf_vector(doc 3 ):  [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print('unique words are: \\n', unique_words, '\\n')\n",
    "for i in range(len(corpus)):\n",
    "    print('tf_vector(doc',i,'): ', tf_vector(corpus[i], corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF document and query representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_frequency(corpus, term):\n",
    "    \"\"\" number of documents that contains given term \"\"\"\n",
    "    count = 0\n",
    "    for doc in corpus:\n",
    "        words = doc.split()\n",
    "        if term in words:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df( finding ):  1\n",
      "df( cancer ):  4\n",
      "df( findings ):  1\n",
      "df( healing ):  1\n",
      "df( new ):  1\n",
      "df( to ):  1\n",
      "df( novel ):  2\n",
      "df( preliminary ):  1\n",
      "df( in ):  1\n",
      "df( heal ):  1\n",
      "df( research ):  4\n"
     ]
    }
   ],
   "source": [
    "for word in unique_words:\n",
    "    print('df(',word,'): ', document_frequency(corpus, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inversed_document_frequency(corpus, term):\n",
    "    \"\"\" logarythm from the number of documents divided by document frequency \"\"\"\n",
    "    import math\n",
    "    df = document_frequency(corpus, term)\n",
    "    return round(math.log10(len(corpus)/df),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf( finding ):  0.602\n",
      "idf( cancer ):  0.0\n",
      "idf( findings ):  0.602\n",
      "idf( healing ):  0.602\n",
      "idf( new ):  0.602\n",
      "idf( to ):  0.602\n",
      "idf( novel ):  0.301\n",
      "idf( preliminary ):  0.602\n",
      "idf( in ):  0.602\n",
      "idf( heal ):  0.602\n",
      "idf( research ):  0.0\n"
     ]
    }
   ],
   "source": [
    "for word in unique_words:\n",
    "    print('idf(',word,'): ', inversed_document_frequency(corpus, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_weight(term, document, corpus):\n",
    "    return round(term_frequency(term, document) * inversed_document_frequency(corpus, term), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_weight('novel', corpus[0], corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_vector(document, corpus):\n",
    "    unique_words = get_unique_words(corpus)\n",
    "    \n",
    "    result = []\n",
    "    for w in unique_words:\n",
    "        result += [tf_idf_weight(w, document, corpus)]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_idf vector for document 0  is:  [0.602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.602, 0.602, 0.0, 0.0]\n",
      "tf_idf vector for document 1  is:  [0.0, 0.0, 0.602, 0.0, 0.0, 0.0, 0.301, 0.0, 0.0, 0.0, 0.0]\n",
      "tf_idf vector for document 2  is:  [0.0, 0.0, 0.0, 0.0, 0.602, 0.602, 0.0, 0.0, 0.0, 0.602, 0.0]\n",
      "tf_idf vector for document 3  is:  [0.0, 0.0, 0.0, 0.602, 0.0, 0.0, 0.301, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    print('tf_idf vector for document',i,' is: ', tf_idf_vector(corpus[i], corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"novel novel preliminary new finding\"\n",
    "q2 = \"healing research\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_idf_vector(q1):  [0.602, 0.0, 0.0, 0.0, 0.602, 0.0, 0.602, 0.602, 0.0, 0.0, 0.0]\n",
      "tf_idf_vector(q2):  [0.0, 0.0, 0.0, 0.602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print('tf_idf_vector(q1): ', tf_idf_vector(q1, corpus))\n",
    "print('tf_idf_vector(q2): ', tf_idf_vector(q2, corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_length(vector):\n",
    "    import math\n",
    "    result = 0\n",
    "    for el in vector:\n",
    "        result += math.pow(el, 2)\n",
    "    return math.sqrt(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(document, query, corpus):\n",
    "    result = 0\n",
    "    d_vec = tf_idf_vector(document, corpus)\n",
    "    q_vec = tf_idf_vector(query, corpus)\n",
    "    \n",
    "    for i in range(len(d_vec)):\n",
    "        result += d_vec[i] * q_vec[i]\n",
    "    \n",
    "    return round(result / (vector_length(d_vec)* vector_length(q_vec)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking for 1st query:\n",
      "['doc0([0.577])', 'doc2([0.289])', 'doc1([0.224])', 'doc3([0.224])']\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "print('Ranking for 1st query:')\n",
    "\n",
    "ranking = dict()\n",
    "for i in range(len(corpus)):\n",
    "    ranking[i] = [cosine_similarity(corpus[i], q1, corpus)]\n",
    "ranking = sorted(ranking.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(['doc' + str(d) + '(' + str(score) + ')' for d, score in ranking])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking for 2nd query:\n",
      "['doc3([0.894])', 'doc0([0.0])', 'doc1([0.0])', 'doc2([0.0])']\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "print('Ranking for 2nd query:')\n",
    "\n",
    "ranking = dict()\n",
    "for i in range(len(corpus)):\n",
    "    ranking[i] = [cosine_similarity(corpus[i], q2, corpus)]\n",
    "ranking = sorted(ranking.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(['doc' + str(d) + '(' + str(score) + ')' for d, score in ranking])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
